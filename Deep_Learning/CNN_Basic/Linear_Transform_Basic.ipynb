{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dd6135",
   "metadata": {},
   "source": [
    "# Linear transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a4028",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear transform란?\n",
    "> [Linear transforms and matrics? 유튜브](https://youtu.be/kYB8IZa5AuE)\n",
    ">> ### 1. 벡터를 개념화하는 방식\n",
    ">> * 0:40 | transform이라는 용어는 근본적으로 function과 같은 말. input을 받고 output을 내놓음.(선형대수 맥락에선 특정 벡터를 받아서 다른 벡터를 내놓음)\n",
    ">> * 1:20 | 어떤 transform이 입력 벡터를 output 벡터로 바꾼다면, 이것을 **입력 벡터를 이동시켜서 출력 벡터로 만드는** 것으로 생각해볼 수 있음\n",
    "![input, output vector](io_vector.png)\n",
    ">> * 1:20 | 그렇다면 모든 가능한 입력벡터에 움직임을 적용하면 그에 상응하는 많은 출력 벡터가 나오겠지만, 모든 벡터의 움직임 상상하는 것은 굉장히 어려움\n",
    ">> * 1:40 | => 그래서 각 벡터를 개념화하는 방법으로 **화살표가 아닌 점(점 하나가 벡터의 끝을 의미)**으로 나타냄. 즉 공간상의 점이 다른 점으로 이동하는 방식으로 생각<br><br>\n",
    ">> ### 2. 다양한 변환이 있지만, linear transform은 특정 조건을 만족하는 변환을 의미함\n",
    ">> * 2:40 | 모든 선은 변환 이후에도 휘지 않고 직선이어야 하며, 원점은 변환 이후에도 여전히 원점이어야 한다.\n",
    ">> * 2:50 | (Linear transform이 아닌 예시)\n",
    ">> * 3:10 | 따라서 **일반적으로 linear transform은 격자 라인이 변형 이후에도 여전히 평행하고 동일한 간격으로 있음**<br><br>\n",
    ">> ### 3. transform을 수치적으로 설명하는 방법\n",
    ">> * 3:50 | 두 개의 기저벡터(i hat, j hat)으로 설명하면 된다.\n",
    ">> * 4:00 | 예를 들어 $\\begin{bmatrix}-1\\\\2\\\\ \\end{bmatrix}$는 $\\vec{v} = -1\\hat{i}+2\\hat{j}$로 볼 수 있음<br><br>\n",
    ">> ### 4. transform 변환 예제\n",
    "![linear_transform_ex1_1](linear_transform_ex1_1.png)\n",
    "![linear_transform_ex1_2](linear_transform_ex1_2.png)\n",
    ">> * 4:20 | 변환 후 $\\vec{v}$는 변환된 i hat 벡터의 -1배, j hat 벡터의 2배가 됨\n",
    ">>> * $\\vec{v} = -1\\hat{i}+2\\hat{j}$\n",
    ">>> * $Transformed\\;\\vec{v} = -1(Transformed\\;\\hat{i})+2(Transformed\\;\\hat{j})$\n",
    ">> * 4:40 | **=> i hat과 j hat의 변형위치만 알면 벡터 $\\vec{v}$를 추론할 수 있다.**\n",
    ">> * 4:50 | i hat이 변환 전 좌표계의 $\\begin{bmatrix}-1\\\\2\\\\ \\end{bmatrix}$로 옮겨지고, y hat이 $\\begin{bmatrix}3\\\\0\\\\ \\end{bmatrix}$으로 옮겨지므로\n",
    ">>> * $Transformed\\;\\vec{v} = -1\\begin{bmatrix}-1\\\\2\\\\ \\end{bmatrix}+2\\begin{bmatrix}3\\\\0\\\\ \\end{bmatrix}$\n",
    ">> * 6:20 | 이 좌표값들은 2x2 행렬로 표현하는 게 일반적임 ($\\begin{bmatrix}3&2\\\\-2&1\\\\ \\end{bmatrix}$)\n",
    ">> * 6:30 | **즉 행렬의 각 컬럼을 i hat 벡터, j hat 벡터가 변환하는 방식을 나타낸 것으로 받아들일 수 있다.**\n",
    ">> * 6:50 | **이것이 변환 후 새 기저 벡터들로 스케일링하고 합한다는 개념**<br><br>\n",
    ">> ### 5. transform 변환 remind\n",
    "![linear_transform_ex2](linear_transform_ex2.png)\n",
    ">> * 7:10 | $\\begin{bmatrix}\\color{green}a&\\color{red}b\\\\\\color{green}c&\\color{red}d\\\\ \\end{bmatrix}$ : 첫번째 열 (a,c)는 첫번째 기저벡터(i hat)의 도착점이고, 두 번째 열 (b,c)는 두번째 기저벡터(j hat)의 도착점이다.\n",
    ">> * 10 20 | **언제나 행렬 계산에서 유의. 공간의 어떤 변환으로 받아들여라** $\\begin{bmatrix}\\color{green}a&\\color{red}b\\\\\\color{green}c&\\color{red}d\\\\ \\end{bmatrix}$     $\\begin{bmatrix}x\\\\y\\\\ \\end{bmatrix}$=$x\\begin{bmatrix}\\color{green}a\\\\\\color{green}c\\\\ \\end{bmatrix}+y\\begin{bmatrix}\\color{red}b\\\\\\color{red}d\\\\ \\end{bmatrix}$\n",
    "![linear_transform_ex3_1](linear_transform_ex3_1.png)\n",
    "![linear_transform_ex3_2](linear_transform_ex3_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369100e",
   "metadata": {},
   "source": [
    "---\n",
    "* 이렇게 Linear transform은 데이터를 특정 차원으로 변환하는 기능을 함.<br>\n",
    "* 100차원의 데이터를 300차원의 데이터로 변환하면 데이터가 더 variable해지고, 반대로 10차원의 데이터로 변환하면 데이터가 집약되는 셈<br>\n",
    " * 예시: 1차원으로 변환하는 데 (2,1) 행렬 $\\begin{bmatrix}\\color{green}a\\\\\\color{green}b\\\\\\end{bmatrix}$, 4차원을 1차원으로 변환하는 데 (4,1) 행렬 $\\begin{bmatrix}\\color{green}a\\\\\\color{green}b\\\\\\color{green}c\\\\\\color{green}d\\\\\\end{bmatrix}$을 사용 // 여기서 행렬이 바로 **Weight**\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ca6db",
   "metadata": {},
   "source": [
    "## 1. Linear transform : 4차원 축소 예시\n",
    ">* 목표 : 두 사각형의 데이터를 집약할 것\n",
    "![두 사각형](image1.png)\n",
    "사각형이므로 (x,y) **2차원의 점 4개로 표현이 가능 => (4,2) 행렬 형태의 데이터로 표현**할 것<br><br>\n",
    ">* 행렬 차원의 변화 과정\n",
    ">>1단계: (4, 2) ⋅ **(2, 1)** = (4, ) // 2차원을 1차원으로 변환<br>\n",
    ">>2단계: (4, ) ⋅ **(4, 1)** = (1, )  // 4차원을 1차원으로 변환<br>\n",
    ">>여기서 **굵은 글씨**가 Weight\n",
    "* ### 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d1f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "1단계 연산 준비: (64, 4, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 16:54:16.193816: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-18 16:54:16.194060: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))     # Tensorflow는 Batch를 기반으로 동작하기에,\n",
    "                                         # 사각형 2개 세트를 batch_size개만큼\n",
    "                                         # 만든 후 처리를 하게 됨\n",
    "print(\"1단계 연산 준비:\", boxes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939d889",
   "metadata": {},
   "source": [
    "#### *모든 원소의 값이 0인 Tensor를 생성했음(tf.zeros를 이용)\n",
    "#### *tf.zeros(shape, dtype=tf.float32, name=None) // shape가 (4, 2)인 0으로 가득 찬 행렬이 64개(batch size)<br>\n",
    "\n",
    "\n",
    "* ### 신경망 layer를 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff23f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 결과: (64, 4)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 1)\n",
      "\n",
      "2단계 연산 준비: (64, 4)\n"
     ]
    }
   ],
   "source": [
    "first_linear = tf.keras.layers.Dense(units=1, use_bias=False) \n",
    "# units은 출력 차원 수를 의미.\n",
    "# Wx+b에서 b에 해당하는 bias가 없으므로 use_bias=False\n",
    "\n",
    "first_out = first_linear(boxes)            # 생성한 행렬을 넣음\n",
    "first_out = tf.squeeze(first_out, axis=-1) # (4, 1)을 (4,)로 변환.\n",
    "                                           # (불필요한 차원 축소)\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6636a445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2단계 연산 결과: (64,)\n",
      "2단계 Linear Layer의 Weight 형태: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af154dde",
   "metadata": {},
   "source": [
    "---\n",
    "#### 하지만 특정 Weight에서 다음과 같은 문제가 발생하게 됨\n",
    "![두 사각형2](image2.png)\n",
    "#### *첫번째 변환에서 두 사각형이 (2,1)형태의 행렬과 만나 (4,)로 변환이 됐지만, 결과가 동일한 경우가 발생.\n",
    "#### *이러면 두번째 변환을 거쳐도 동일한 결과를 얻으니 의미가 없음.\n",
    "=> 따라서 데이터를 풍부하게 해줄 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b6d05",
   "metadata": {},
   "source": [
    "---\n",
    "### 1-2. 데이터가 풍부해지도록 Weight를 조정\n",
    ">* 식2의 행렬 차원의 변화 과정\n",
    ">>1단계: (4, 2) ⋅ **(2, 1)** = (4, ) // 2차원을 1차원으로 변환<br>\n",
    ">>2단계: (4, ) ⋅ **(4, 1)** = (1, )  // 4차원을 1차원으로 변환<br>\n",
    ">>여기서 **굵은 글씨**가 Weight\n",
    "\n",
    "\n",
    "\n",
    "![두 사각형2](image3.png)\n",
    "#### *두 결과가 독립적인 결과를 가지므로 구분이 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f3f55",
   "metadata": {},
   "source": [
    "* ### 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7988af75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 준비: (64, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))\n",
    "\n",
    "print(\"1단계 연산 준비:\", boxes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc29d13",
   "metadata": {},
   "source": [
    "* ### 신경망 layer를 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e9ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 결과: (64, 4, 3)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 3)\n",
      "\n",
      "2단계 연산 준비: (64, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "first_linear = tf.keras.layers.Dense(units=3, use_bias=False)\n",
    "first_out = first_linear(boxes)\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac87d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2단계 연산 결과: (64, 4)\n",
      "2단계 Linear Layer의 Weight 형태: (3, 1)\n",
      "\n",
      "3단계 연산 준비: (64, 4)\n"
     ]
    }
   ],
   "source": [
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n3단계 연산 준비:\", second_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd6dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3단계 연산 결과: (64,)\n",
      "3단계 Linear Layer의 Weight 형태: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "third_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "third_out = third_linear(second_out)\n",
    "third_out = tf.squeeze(third_out, axis=-1)\n",
    "\n",
    "print(\"3단계 연산 결과:\", third_out.shape)\n",
    "print(\"3단계 Linear Layer의 Weight 형태:\", third_linear.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca9104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 Parameters: 13\n"
     ]
    }
   ],
   "source": [
    "total_params = \\\n",
    "first_linear.count_params() + \\\n",
    "second_linear.count_params() + \\\n",
    "third_linear.count_params()\n",
    "\n",
    "print(\"총 Parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d3360",
   "metadata": {},
   "source": [
    "#### *하지만 너무 많은 Parameter는 과적합(Overfitting)을 야기할 수 있으므로 유의해야 한다.<br><br>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "* ### 편향(Bias)\n",
    "![bias](bias.png)\n",
    "#### 두 데이터는 비슷하게 생겼지만, 원점을 건드리지 않고는 둘을 일치시키기 어려움. 단순히 원점을 평행이동시키는 것만으로 해결이 가능하다면 bias를 이용하면 될 것.\n",
    "\n",
    "\n",
    "=> y = Wx + b의 b가 바로 bias\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
