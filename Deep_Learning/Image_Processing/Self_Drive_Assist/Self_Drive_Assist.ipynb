{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c778ea",
   "metadata": {},
   "source": [
    "# 자율주행 시스템 만들기: RetinaNet\n",
    "2017년 나온 RetinaNet은 focal loss와 FPN(Feature Pyramid Network) 를 적용한 네트워크를 사용함.\n",
    ">[Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "* 물체를 배경보다 더 잘 학습하자 == 물체인 경우 Loss를 작게 만들자 라는 발상\n",
    "* 사실 대부분의 이미지에서는 물체보다 배경이 많음. 따라서 이미지는 극단적으로 배경의 class가 많은 class imbalanced data인 점을 생각.<br><br>\n",
    "                                                        \n",
    "                                                        \n",
    "#### 1. Focal Loss :\n",
    "기존의 1-stage detection 모델들(YOLO, SSD)이 물체 전경과 배경을 담고 있는 모든 그리드(grid)에 대해 한 번에 학습됨으로 인해서 생기는 클래스 간의 불균형을 해결하고자 도입됨\n",
    "![focal loss](focal_loss.png)\n",
    "* focal loss는 교차 엔트로피를 기반으로 만들어짐. 단순히 교차 엔트로피 CE($p_{t}$) 앞단에 간단히 $(1-p_{t})^{\\gamma}$라는 modulating factor를 붙여줌\n",
    "* 대체로 이미지는 배경이 많으므로, 너무 많은 배경 class에 압도되지 않도록 modulation factor로 loss를 조절하는 것.\n",
    "* $\\gamma$를 0으로 설정하면 modulation factor $(1-p_{t})^{\\gamma}$가 1이 되어 일반적인 교차 엔트로피가 됨.\n",
    "* $\\gamma$를 크게 설정할수록 modulating이 강하게 적용됨.<br><br>\n",
    "\n",
    "\n",
    "\n",
    "#### 2. FPN(Feature Pyramid Network) :\n",
    "![FPN](FPN.png)\n",
    "* FPN은 특성을 피라미드처럼 쌓아서 사용하는 방식.\n",
    "* CNN 백본 네트워크에서는 다양한 레이어의 결과값을 특성 맵(feature map)으로 사용할 수 있는데, 이때 컨볼루션 연산은 커널을 통해 일정한 영역을 보고 몇 개의 숫자로 요약해 내기 때문에, 입력 이미지를 기준으로 생각하면 입력 이미지와 먼 모델의 뒷쪽의 특성 맵일수록 하나의 \"셀(cell)\"이 넓은 이미지 영역의 정보(receptive field)를 담음.\n",
    "* RetinaNet에서는 **receptive field가 넓은 뒷쪽의 특성 맵을 upsampling(확대)**하여 **앞단의 특성 맵과 더해서** 사용.\n",
    "* FPN은 백본의 여러 레이어를 한꺼번에 쓰겠다라는데에 의의가 있는 것.\n",
    "\n",
    "\n",
    "![feature pyramid net](FPN_2.png)\n",
    "* RetinaNet에서는 FPN을 통해 $P_{3}$부터 $P_{7}$까지의 pyramid level을 생성해 사용함.\n",
    "* 각 pyramid level은 256개의 채널로 이루어짐. \n",
    "* 이를 통해 Classification Subnet과 Box Regression Subnet 2개의 Subnet을 구성하게 되는데, Anchor 갯수를 AA라고 하면 최종적으로 Classification Subnet은 KK개 class에 대해 KAKA개 채널을, Box Regression Subnet은 4A4A개 채널을 사용하게 됨.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d9132",
   "metadata": {},
   "source": [
    "#### 요구사항\n",
    "* 사람이 카메라에 감지되면 정지\n",
    "* 차량이 일정 크기 이상으로 감지되면 정지\n",
    "\n",
    "\n",
    "tensorflow_datasets에서 제공하는 KITTI 데이터셋을 사용할 것.\n",
    ">[cvlibs에서 제공하는 KITTI 데이터셋](http://www.cvlibs.net/datasets/kitti/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3fba9",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5bb98",
   "metadata": {},
   "source": [
    "```Python\n",
    "import os, copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "DATA_PATH = os.getenv('HOME') + '/Aiffel_Project/Data'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd642a",
   "metadata": {},
   "source": [
    "```Python\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'kitti',\n",
    "    data_dir=DATA_PATH,\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f592b8a",
   "metadata": {},
   "source": [
    "## 2. 데이터 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0c2b5",
   "metadata": {},
   "source": [
    "#### tfds.show_examples을 통해 데이터셋을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3558a",
   "metadata": {},
   "source": [
    "```Python\n",
    "_ = tfds.show_examples(ds_train, ds_info)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8204335",
   "metadata": {},
   "source": [
    "#### ds_info를 통해 데이터셋의 정보를 파악\n",
    "* 데이터셋은 6,347개의 학습 데이터(training data), 711개의 평가용 데이터(test data), 423개의 검증용 데이터(validation data)로 구성\n",
    "* 라벨은 alpha, bbox, dimensions, location, occluded, rotation_y, truncated 등의 정보로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d6ae3",
   "metadata": {},
   "source": [
    "```Python\n",
    "ds_info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e1a3a",
   "metadata": {},
   "source": [
    "#### 데이터 직접 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f397f84",
   "metadata": {},
   "source": [
    "```Python\n",
    "sample = ds_train.take(1)\n",
    "\n",
    "for example in sample:  \n",
    "    print('------Example------')\n",
    "    print(list(example.keys()))\n",
    "    image = example[\"image\"]\n",
    "    filename = example[\"image/file_name\"].numpy().decode('utf-8')\n",
    "    objects = example[\"objects\"]\n",
    "\n",
    "print('------objects------')\n",
    "print(objects)\n",
    "\n",
    "img = Image.fromarray(image.numpy())\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a3875a",
   "metadata": {},
   "source": [
    "#### 바운딩 박스 그려서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f62a3",
   "metadata": {},
   "source": [
    "```Python\n",
    "def visualize_bbox(input_image, object_bbox):\n",
    "    input_image = copy.deepcopy(input_image)\n",
    "    draw = ImageDraw.Draw(input_image)\n",
    "    \n",
    "    # 바운딩 박스 좌표(x_min, x_max, y_min, y_max) 구하기\n",
    "    width, height = img.size\n",
    "    x_min = object_bbox[:,1] * width\n",
    "    x_max = object_bbox[:,3] * width\n",
    "    y_min = height - object_bbox[:,0] * height\n",
    "    y_max = height - object_bbox[:,2] * height\n",
    "    \n",
    "    # 바운딩 박스 그리기\n",
    "    rects = np.stack([x_min, y_min, x_max, y_max], axis=1)\n",
    "    for _rect in rects:\n",
    "        draw.rectangle(_rect, outline=(255,0,0), width=2)\n",
    "\n",
    "    return input_image\n",
    "\n",
    "visualize_bbox(img, objects['bbox'].numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8df128",
   "metadata": {},
   "source": [
    "## 3. 데이터 파이프라인 만들기\n",
    "* x와 y좌표 위치 교체\n",
    "* 무작위로 수평 뒤집기(Flip)\n",
    "* 이미지 크기 조정 및 패딩 추가\n",
    "* 좌표계를 [x_min, y_min, x_max, y_max]에서 [x_min, y_min, width, height]으로 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920aa4bd",
   "metadata": {},
   "source": [
    "```Python\n",
    "# x와 y좌표 교체\n",
    "\n",
    "def swap_xy(boxes):\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86cf63",
   "metadata": {},
   "source": [
    "```Python\n",
    "# 무작위로 수평 flip\n",
    "\n",
    "def random_flip_horizontal(image, boxes):\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack(\n",
    "           [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
    "        )\n",
    "        \n",
    "    return image, boxes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fb289",
   "metadata": {},
   "source": [
    "```Python\n",
    "# 이미지 크기 조정 및 패딩 추가\n",
    "# 이미지의 비율은 그대로 유지되어야 하고, 이미지의 최대/최소 크기도 제한해야 함\n",
    "# 또 이미지의 크기를 바꾼 후에도 최종적으로 모델에 입력되는 이미지의 크기는 stride의 배수가 되도록 할 것\n",
    "\n",
    "def resize_and_pad_image(image, training=True):\n",
    "\n",
    "    min_side = 800.0\n",
    "    max_side = 1333.0\n",
    "    min_side_range = [640, 1024]\n",
    "    stride = 128.0\n",
    "    \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if training:\n",
    "        min_side = tf.random.uniform((), min_side_range[0], min_side_range[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(\n",
    "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
    "    )\n",
    "    return image, image_shape, ratio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dadb5",
   "metadata": {},
   "source": [
    "```Python\n",
    "# 좌표계를 [x_min, y_min, x_max, y_max]에서 [x_min, y_min, width, height]으로 수정\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a90368",
   "metadata": {},
   "source": [
    "#### 준비한 함수들을 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47e12a",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "def preprocess_data(sample):\n",
    "    image = sample[\"image\"]\n",
    "    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
    "    class_id = tf.cast(sample[\"objects\"][\"type\"], dtype=tf.int32)\n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)\n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)\n",
    "    return image, bbox, class_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa618f",
   "metadata": {},
   "source": [
    "## 4. 인코딩\n",
    "#### Anchor Box 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07633b",
   "metadata": {},
   "source": [
    "```Python\n",
    "class AnchorBox:\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(3, 8)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(3, 8)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a2370",
   "metadata": {},
   "source": [
    "#### IoU를 계산할 수 있는 함수를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63abf4",
   "metadata": {},
   "source": [
    "```Python\n",
    "def convert_to_corners(boxes):\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce5d56",
   "metadata": {},
   "source": [
    "#### 이미지가 최대한 Anchor Box와 맞게 맞춰주는 클래스 생성\n",
    "* compute_iou 함수를 이용해서 IoU를 구하고, 그 IoU를 기준으로 물체에 해당하는 Anchor Box와 배경이 되는 Anchor Box를 지정.\n",
    "* 그 Anchor Box와 실제 Bounding Box의 미세한 차이를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70637296",
   "metadata": {},
   "source": [
    "```Python\n",
    "class LabelEncoder:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
    "    ):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
    "        )\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            labels = labels.write(i, label)\n",
    "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
    "        return batch_images, labels.stack()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad230a2",
   "metadata": {},
   "source": [
    "### 4. 모델 작성: Feature Pyramid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5a9eb",
   "metadata": {},
   "source": [
    "```Python\n",
    "class FeaturePyramid(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, backbone):\n",
    "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\")\n",
    "        self.backbone = backbone\n",
    "        self.conv_c3_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c5_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c3_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c5_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "        self.conv_c7_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "        self.upsample_2x = tf.keras.layers.UpSampling2D(2)\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
    "        p3_output = self.conv_c3_1x1(c3_output)\n",
    "        p4_output = self.conv_c4_1x1(c4_output)\n",
    "        p5_output = self.conv_c5_1x1(c5_output)\n",
    "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
    "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
    "        p3_output = self.conv_c3_3x3(p3_output)\n",
    "        p4_output = self.conv_c4_3x3(p4_output)\n",
    "        p5_output = self.conv_c5_3x3(p5_output)\n",
    "        p6_output = self.conv_c6_3x3(c5_output)\n",
    "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "        return p3_output, p4_output, p5_output, p6_output, p7_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2943da1",
   "metadata": {},
   "source": [
    "#### Object Detection의 라벨은 class와 box로 이루어지므로 각각을 추론하는 부분이 필요\n",
    "* class를 예측하는 head와 box를 예측하는 head가 별도로 존재한다는 것이 중요\n",
    "* build_head라는 함수를 하나만 만들고 두 번 호출할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673d890",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "def build_head(output_filters, bias_init):\n",
    "    head = tf.keras.Sequential([tf.keras.Input(shape=[None, None, 256])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
    "    for _ in range(4):\n",
    "        head.add(\n",
    "            tf.keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
    "        )\n",
    "        head.add(tf.keras.layers.ReLU())\n",
    "    head.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            output_filters,\n",
    "            3,\n",
    "            1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            bias_initializer=bias_init,\n",
    "        )\n",
    "    )\n",
    "    return head\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff020a",
   "metadata": {},
   "source": [
    "RetinaNet의 backbone은 ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38f29f",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "def get_backbone():\n",
    "    backbone = tf.keras.applications.ResNet50(\n",
    "        include_top=False, input_shape=[None, None, 3]\n",
    "    )\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
    "    ]\n",
    "    return tf.keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72242b",
   "metadata": {},
   "source": [
    "RetinaNet: Backbone + FPN + classification용 head + box용 head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e8f71",
   "metadata": {},
   "source": [
    "```Python\n",
    "class RetinaNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes, backbone):\n",
    "        super(RetinaNet, self).__init__(name=\"RetinaNet\")\n",
    "        self.fpn = FeaturePyramid(backbone)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(9 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=False):\n",
    "        features = self.fpn(image, training=training)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "        for feature in features:\n",
    "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
    "            cls_outputs.append(\n",
    "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
    "            )\n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        return tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961c183",
   "metadata": {},
   "source": [
    "## 4. 손실 함수 정의\n",
    "* Focal Loss는 Box Regression에는 사용하지 않고 Classification Loss를 계산하는데만 사용.\n",
    "* Box Regression에는 Smooth L1 Loss를 씀\n",
    "* Smooth L1 Loss을 사용하는 Box Regression에는 delta를 기준으로 계산이 달라지고, Focal Loss를 사용하는 Classification에서는 alpha와 gamma를 사용해서 물체일 때와 배경일 때의 식이 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc34f0",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred\n",
    "        )\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "\n",
    "    def __init__(self, num_classes=8, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        loss = clf_loss + box_loss\n",
    "        return loss\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fce3f",
   "metadata": {},
   "source": [
    "### 5. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ba59b",
   "metadata": {},
   "source": [
    "```Python\n",
    "num_classes = 8\n",
    "batch_size = 2\n",
    "\n",
    "resnet50_backbone = get_backbone()\n",
    "loss_fn = RetinaNetLoss(num_classes)\n",
    "model = RetinaNet(num_classes, resnet50_backbone)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fb7c5",
   "metadata": {},
   "source": [
    "```Python\n",
    "# learning rate 설정\n",
    "\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")\n",
    "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9bc5e2",
   "metadata": {},
   "source": [
    "#### 전처리를 위한 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e527088",
   "metadata": {},
   "source": [
    "```Python\n",
    "label_encoder = LabelEncoder()\n",
    "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
    "    \"kitti\", split=[\"train\", \"validation\"], with_info=True, data_dir=DATA_PATH\n",
    ")\n",
    "\n",
    "autotune = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "    batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
    ")\n",
    "train_dataset = train_dataset.prefetch(autotune)\n",
    "\n",
    "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(\n",
    "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    ")\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.prefetch(autotune)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118cac9",
   "metadata": {},
   "source": [
    "```Python\n",
    "model_dir = os.getenv('HOME') + '/Aiffel_Project/Deep_Learning/Image_Processing/Self_Drive_Assist/checkpoints/'\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433bc7a",
   "metadata": {},
   "source": [
    "### 6. 결과 확인\n",
    "모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787ac29",
   "metadata": {},
   "source": [
    "```Python\n",
    "model_dir = os.getenv('HOME') + '/Aiffel_Project/Deep_Learning/Image_Processing/Self_Drive_Assist/checkpoints/'\n",
    "latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
    "model.load_weights(latest_checkpoint)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa9669",
   "metadata": {},
   "source": [
    "NMS(Non-Max Suppression)은 tf.image.combined_non_max_suppression 사용.\n",
    ">[tf.image.combined_non_max_suppression](https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198997e",
   "metadata": {},
   "source": [
    "```Python\n",
    "class DecodePredictions(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=8,\n",
    "        confidence_threshold=0.05,\n",
    "        nms_iou_threshold=0.5,\n",
    "        max_detections_per_class=100,\n",
    "        max_detections=100,\n",
    "        box_variance=[0.1, 0.1, 0.2, 0.2]\n",
    "    ):\n",
    "        super(DecodePredictions, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.nms_iou_threshold = nms_iou_threshold\n",
    "        self.max_detections_per_class = max_detections_per_class\n",
    "        self.max_detections = max_detections\n",
    "\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            box_variance, dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
    "        boxes = box_predictions * self._box_variance\n",
    "        boxes = tf.concat(\n",
    "            [\n",
    "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        boxes_transformed = convert_to_corners(boxes)\n",
    "        return boxes_transformed\n",
    "\n",
    "    def call(self, images, predictions):\n",
    "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        box_predictions = predictions[:, :, :4]\n",
    "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
    "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "\n",
    "        return tf.image.combined_non_max_suppression(\n",
    "            tf.expand_dims(boxes, axis=2),\n",
    "            cls_predictions,\n",
    "            self.max_detections_per_class,\n",
    "            self.max_detections,\n",
    "            self.nms_iou_threshold,\n",
    "            self.confidence_threshold,\n",
    "            clip_boxes=False,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec782669",
   "metadata": {},
   "source": [
    "#### 모델 조립"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4375ce",
   "metadata": {},
   "source": [
    "```Python\n",
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=detections)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d80c97",
   "metadata": {},
   "source": [
    "#### 모델 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79947d5c",
   "metadata": {},
   "source": [
    "```Python\n",
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
    "):\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box, _cls, score in zip(boxes, classes, scores):\n",
    "        text = \"{}: {:.2f}\".format(_cls, score)\n",
    "        x1, y1, x2, y2 = box\n",
    "        origin_x, origin_y = x1, image.shape[0] - y2 # matplitlib에서 Rectangle와 text를 그릴 때는 좌하단이 원점이고 위로 갈 수록 y값이 커집니다\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        patch = plt.Rectangle(\n",
    "            [origin_x, origin_y], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        ax.text(\n",
    "            origin_x,\n",
    "            origin_y,\n",
    "            text,\n",
    "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
    "            clip_box=ax.clipbox,\n",
    "            clip_on=True,\n",
    "        )\n",
    "    plt.show()\n",
    "    return ax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca0dc8",
   "metadata": {},
   "source": [
    "```Python\n",
    "def prepare_image(image):\n",
    "    image, _, ratio = resize_and_pad_image(image, training=False)\n",
    "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "    return tf.expand_dims(image, axis=0), ratio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c58a67",
   "metadata": {},
   "source": [
    "#### 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273f4f0",
   "metadata": {},
   "source": [
    "```Python\n",
    "test_dataset = tfds.load(\"kitti\", split=\"test\", data_dir=DATA_PATH)\n",
    "int2str = dataset_info.features[\"objects\"][\"type\"].int2str\n",
    "\n",
    "for sample in test_dataset.take(2):\n",
    "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
    "    input_image, ratio = prepare_image(image)\n",
    "    detections = inference_model.predict(input_image)\n",
    "    num_detections = detections.valid_detections[0]\n",
    "    class_names = [\n",
    "        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
    "    ]\n",
    "    visualize_detections(\n",
    "        image,\n",
    "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
    "        class_names,\n",
    "        detections.nmsed_scores[0][:num_detections],\n",
    "    )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
