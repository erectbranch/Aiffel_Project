{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833e4ca0",
   "metadata": {},
   "source": [
    "# 자연어 처리를 위한 개념 정리\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df3f15",
   "metadata": {},
   "source": [
    "* ### 희소 표현(Sparse Representation)\n",
    " 벡터의 특정 차원에 단어 혹은 의미를 __직접 매핑__하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed9c18",
   "metadata": {},
   "source": [
    "* ### 분포 가설(distribution hypothesis)\n",
    "유사한 맥락에서 나타나는 단어는 그 의미도 비슷하다\n",
    ">나는 밥을 먹는다.<br>\n",
    ">나는 떡을 먹는다.<br>\n",
    ">나는 _____을 먹는다.<br>\n",
    "\n",
    "    위와 같이 **유사한 맥락**에서 나오는 단어들끼리는 두 단어 벡터 사이의 거리를 가깝게 하고, 그렇지 않은 단어는 멀어지도록 조정한다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "=> 따라서 분산 표현을 사용하면 희소 표현과 다르게 **단어 간의 유사도**를 계산할 수 있음<br><br>\n",
    "###     <span style=\"color:red\">Embedding 레이어가 바로 단어의 분산 표현을 구하기 위한 레이어.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826a1cd",
   "metadata": {},
   "source": [
    "## 1. Embedding Layer 기초\n",
    " **'단어 n개를 사용하고 k차원으로 표현할 것'**이라고 전달하면 알아서 **분산 표현 사전을 구성**해 준다.\n",
    " ![Embedding Layer](embedding_layer.png)\n",
    "\n",
    "### Weight는 자연스럽게 '단어의 개수', 'Embedding 사이즈'로 정의됨<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4040a",
   "metadata": {},
   "source": [
    " * ### 또한 Embedding Layer는 원-핫 인코딩(One-Hot Encoding)과 결합하여 쓰임\n",
    "\"i i i i need some more coffee coffee coffee\"란 문장을 원핫 인코딩을 적용해서 임베딩할 것.<br>\n",
    "vocab(단어 사전)을 유의해서 볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e947a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 20:05:53.264018: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-19 20:05:53.264143: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding 이해를 위한 간단한 예제\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab = {      # 사용할 단어 사전 정의\n",
    "    \"i\": 0,\n",
    "    \"need\": 1,\n",
    "    \"some\": 2,\n",
    "    \"more\": 3,\n",
    "    \"coffee\": 4,\n",
    "    \"cake\": 5,\n",
    "    \"cat\": 6,\n",
    "    \"dog\": 7\n",
    "}\n",
    "\n",
    "sentence = \"i i i i need some more coffee coffee coffee\"\n",
    "# 위 sentence\n",
    "_input = [vocab[w] for w in sentence.split()]  # [0, 0, 0, 0, 1, 2, 3, 4, 4, 4]\n",
    "\n",
    "vocab_size = len(vocab)   # 8\n",
    "\n",
    "one_hot = tf.one_hot(_input, vocab_size)\n",
    "print(one_hot.numpy())    # 원-핫 인코딩 벡터를 출력해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae50b9",
   "metadata": {},
   "source": [
    "#### 이 결과를 Linear Layer에 넣을 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fe9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Weight\n",
      "[[-0.02925324 -0.66063195]\n",
      " [ 0.22809005 -0.67404604]\n",
      " [-0.01334894  0.19311988]\n",
      " [ 0.07958895 -0.14330125]\n",
      " [ 0.6256676   0.24200141]\n",
      " [ 0.42546546 -0.60430086]\n",
      " [-0.34569496 -0.66989154]\n",
      " [ 0.38482547  0.15331137]]\n",
      "\n",
      "One-Hot Linear Result\n",
      "[[-0.02925324 -0.66063195]\n",
      " [-0.02925324 -0.66063195]\n",
      " [-0.02925324 -0.66063195]\n",
      " [-0.02925324 -0.66063195]\n",
      " [ 0.22809005 -0.67404604]\n",
      " [-0.01334894  0.19311988]\n",
      " [ 0.07958895 -0.14330125]\n",
      " [ 0.6256676   0.24200141]\n",
      " [ 0.6256676   0.24200141]\n",
      " [ 0.6256676   0.24200141]]\n"
     ]
    }
   ],
   "source": [
    "distribution_size = 2   # 보기 좋게 2차원으로 분산 표현\n",
    "linear = tf.keras.layers.Dense(units=distribution_size, use_bias=False)\n",
    "one_hot_linear = linear(one_hot)\n",
    "\n",
    "print(\"Linear Weight\")\n",
    "print(linear.weights[0].numpy())\n",
    "\n",
    "print(\"\\nOne-Hot Linear Result\")\n",
    "print(one_hot_linear.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74696877",
   "metadata": {},
   "source": [
    "=> 원-핫 벡터에 Linear 레이어를 적용하니 **Linear 레이어의 Weight에서 단어 인덱스 배열 [ 0, 0, 0, 0, 1, 2, 3, 4, 4, 4 ] 에 해당하는 행만 읽어옴**\n",
    " ![Embedding Layer](embedding_layer.png)\n",
    " #### 즉 그림에서 파란색 선 과정 =  각 단어를 원-핫 인코딩해서 Linear 연산하는 것을 의미함<br><br>\n",
    "\n",
    "\n",
    "\n",
    "### + 주의점 : Embedding 레이어는 그저 단어를 대응 시켜 줄 뿐이니 미분이 불가능. \n",
    "    따라서 신경망 설계를 할 때, 어떤 연산 결과를 Embedding 레이어에 연결시킬 수 없다.\n",
    "    Embedding Layer를 입력에 직접 연결되게 사용하는 방식으로 이용\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbad75cd",
   "metadata": {},
   "source": [
    "## 2. Embedding Layer의 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dde408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding을 진행할 문장: (1, 3)\n",
      "Embedding된 문장: (1, 3, 100)\n",
      "Embedding Layer의 Weight 형태: (64, 100)\n"
     ]
    }
   ],
   "source": [
    "some_words = tf.constant([[3, 57, 35]])\n",
    "# 3번 단어 / 57번 단어 / 35번 단어로 이루어진 한 문장으로 생각.\n",
    "\n",
    "print(\"Embedding을 진행할 문장:\", some_words.shape)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=64, output_dim=100)\n",
    "# 총 64개의 단어를 포함한 Embedding 레이어를 선언할 것이며,\n",
    "# 각 단어는 100차원으로 분산 표현 할 것.\n",
    "\n",
    "print(\"Embedding된 문장:\", embedding_layer(some_words).shape)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb603e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Recurrent Layer\n",
    "문장이나 영상, 음성 등의 데이터는 한 장의 이미지 데이터와는 사뭇 다른 특성을 가짐. 바로 **순차적인(Sequential) 특성**\n",
    "> 나는 밥을 [ ]는다.<br>\n",
    ">> 앞에 나온 밥이라는 단어 때문에 '밥'이란 단어가 들어갈 것을 알 수 있음.\n",
    "\n",
    "\n",
    "> 데이터의 나열 사이에 연관성이 없다고 해서 순차적인 데이터가 아니라고 할 수는 없음. [1, 2, 3, 오리, baby, 0.7] 라는 데이터도 요소들 간의 연관성이 없지만 시퀀스 데이터\n",
    "\n",
    "###     <span style=\"color:red\">이런 순차 데이터를 처리하기 위해 고안된 것이 바로 Recurrent Neural Network 또는 Recurrent 레이어(RNN)</span>\n",
    "![RNN](RNN.png)\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8123fb1",
   "metadata": {},
   "source": [
    "#### 하지만 RNN은 다음과 같은 문제점을 가지고 있음\n",
    "![RNN_illustrated](RNN_problem.png)\n",
    "* 첫 입력인 What의 정보가 마지막 입력인 ?에 다다라서는 **거의 희석된 모습**이 보임\n",
    "### 이를 기울기 소실(Vanishing Gradient) 문제라고 함\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5c528",
   "metadata": {},
   "source": [
    "## 4. RNN의 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071dace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN에 입력할 문장: What time is it ?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it ?\"\n",
    "dic = {\n",
    "    \"is\": 0,\n",
    "    \"it\": 1,\n",
    "    \"What\": 2,\n",
    "    \"time\": 3,\n",
    "    \"?\": 4\n",
    "}\n",
    "\n",
    "print(\"RNN에 입력할 문장:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3884b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding을 위해 단어 매핑: [[2 3 0 1 4]]\n",
      "입력 문장 데이터 형태: (1, 5)\n",
      "\n",
      "Embedding 결과: (1, 5, 100)\n",
      "Embedding Layer의 Weight 형태: (5, 100)\n"
     ]
    }
   ],
   "source": [
    "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
    "\n",
    "print(\"Embedding을 위해 단어 매핑:\", sentence_tensor.numpy())\n",
    "print(\"입력 문장 데이터 형태:\", sentence_tensor.shape)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)\n",
    "emb_out = embedding_layer(sentence_tensor)\n",
    "\n",
    "print(\"\\nEmbedding 결과:\", emb_out.shape)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29cbfb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RNN 결과 (모든 Step Output): (1, 5, 64)\n",
      "RNN Layer의 Weight 형태: (100, 64)\n",
      "\n",
      "RNN 결과 (최종 Step Output): (1, 64)\n",
      "RNN Layer의 Weight 형태: (100, 64)\n"
     ]
    }
   ],
   "source": [
    "rnn_seq_layer = \\\n",
    "tf.keras.layers.SimpleRNN(units=64, return_sequences=True, use_bias=False)\n",
    "rnn_seq_out = rnn_seq_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (모든 Step Output):\", rnn_seq_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_seq_layer.weights[0].shape)\n",
    "\n",
    "rnn_fin_layer = tf.keras.layers.SimpleRNN(units=64, use_bias=False)\n",
    "rnn_fin_out = rnn_fin_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (최종 Step Output):\", rnn_fin_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_fin_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497a21b",
   "metadata": {},
   "source": [
    ">어떤 문장이 부정, 긍정인지 파악하는 것은 문장을 모두 읽은 후, 최종 Step의 Output만 확인해도 판단이 가능. <br>\n",
    ">하지만 문장을 생성하는 경우라면 이전 단어를 입력으로 받아 생성된 모든 다음 단어, 즉 모든 Step에 대한 Output이 필요하다.\n",
    "\n",
    "\n",
    "=> 이는 tf.keras.layers.SimpleRNN 레이어의 return_sequences 인자를 조절해서 해결이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57741c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "\n",
      "LSTM 결과 (모든 Step Output): (1, 5, 64)\n",
      "LSTM Layer의 Weight 형태: (100, 256)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "\n",
      "LSTM 결과 (최종 Step Output): (1, 64)\n",
      "LSTM Layer의 Weight 형태: (100, 256)\n"
     ]
    }
   ],
   "source": [
    "lstm_seq_layer = tf.keras.layers.LSTM(units=64, return_sequences=True, use_bias=False)\n",
    "lstm_seq_out = lstm_seq_layer(emb_out)\n",
    "\n",
    "print(\"\\nLSTM 결과 (모든 Step Output):\", lstm_seq_out.shape)\n",
    "print(\"LSTM Layer의 Weight 형태:\", lstm_seq_layer.weights[0].shape)\n",
    "\n",
    "lstm_fin_layer = tf.keras.layers.LSTM(units=64, use_bias=False)\n",
    "lstm_fin_out = lstm_fin_layer(emb_out)\n",
    "\n",
    "print(\"\\nLSTM 결과 (최종 Step Output):\", lstm_fin_out.shape)\n",
    "print(\"LSTM Layer의 Weight 형태:\", lstm_fin_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2bfc9",
   "metadata": {},
   "source": [
    "#### 이것이 바로 기울기 소실(Vanishing Gradient)에 의해 장기 의존성(Long-Term Dependency)을 잘 다루지 못하는 문제를 해결하기 위해 등장한 LSTM\n",
    "\n",
    "\n",
    "---\n",
    "## 5. LSTM의 구조\n",
    "\n",
    "\n",
    "![RNN&LSTM](RNN_LSTM.png)\n",
    "<center>(좌측: RNN, 우측: LSTM)</center>\n",
    "\n",
    "\n",
    "![LSTM](LSTM.png)\n",
    "<center>(LSTM 구조 그래프)</center>\n",
    "\n",
    "\n",
    "* $c_t$ : Cell state의 약자로 long-term memory 역할을 수행함\n",
    "* Forget Gate Layer : cell state의 기존 정보를 얼마나 잊어버릴지를 결정하는 gate\n",
    "* Input Gate Layer : 새롭게 들어온 정보를 기존 cell state에 얼마나 반영할지를 결정하는 gate\n",
    "* Output Gate Layer : 새롭게 만들어진 cell state를 새로운 hidden state에 얼마나 반영할지를 결정하는 gate<br><br>\n",
    "\n",
    "\n",
    "### 5-1. 변형 LSTM\n",
    "* Gated Recurrent Units(GRU)\n",
    "> [Gated Recurrent Units(GRU)](https://yjjo.tistory.com/18)\n",
    "\n",
    "\n",
    "![GRU](GRU.png)\n",
    "<center>(GRU 구조 그래프)</center>\n",
    "\n",
    "\n",
    "LSTM은 GRU에 비해 Weight가 많기 때문에 충분한 데이터가 있는 상황에 적합하고, 반대로 GRU는 적은 데이터에도 웬만한 학습 성능을 보여줌<br><br>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c416ad2",
   "metadata": {},
   "source": [
    "## 6. 양방향(Bidirectional) RNN\n",
    "> 날이 너무 <span style=\"color:orange\">[?]</span> 에어컨을 켰다 \n",
    "\n",
    "\n",
    "이런 예문이 있다면, 빈칸에 들어갈 말이 <span style=\"color:orange\">'더워서'</span>인 것은 어렵지 않게 유추할 수 있음.<br>\n",
    "하지만 **뒤에 나오는** '에어컨을 켰다'라는 말이 있기 때문에 유추가 가능했던 것으로, **순방향으로 진행되는 RNN은 이 정보를 모른 채로** 단어를 생성하게 됨.<br>\n",
    "(날이 너무 추워서 에어컨을 켰다 같은 이상한 문장이 탄생할 수 있음)<br><br>\n",
    "\n",
    "\n",
    "###     <span style=\"color:red\">이런 문제를 해결하기 위해 고안된 것이 바로 양방향(Bidirectional) RNN</span>\n",
    "(사실 단순히 진행 방향이 반대인 RNN을 2개 겹쳐놓은 형태이다.) : tf.keras.layers.Bidirectional()<br><br>\n",
    "\n",
    "\n",
    "#### 사실 자연어 처리는 대체로 번역기를 만들 때 양방향(Bidirectional) RNN 계열의 네트워크, 혹은 동일한 효과를 내는 Transformer 네트워크를 주로 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ee4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sentence = \"What time is it ?\"\n",
    "dic = {\n",
    "    \"is\": 0,\n",
    "    \"it\": 1,\n",
    "    \"What\": 2,\n",
    "    \"time\": 3,\n",
    "    \"?\": 4\n",
    "}\n",
    "\n",
    "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a012c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장 데이터 형태: (1, 5, 100)\n",
      "Bidirectional RNN 결과 (최종 Step Output): (1, 5, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)\n",
    "emb_out = embedding_layer(sentence_tensor)\n",
    "\n",
    "print(\"입력 문장 데이터 형태:\", emb_out.shape)\n",
    "\n",
    "bi_rnn = \\\n",
    "tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.SimpleRNN(units=64, use_bias=False, return_sequences=True)\n",
    ")\n",
    "bi_out = bi_rnn(emb_out)\n",
    "\n",
    "print(\"Bidirectional RNN 결과 (최종 Step Output):\", bi_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebfd65",
   "metadata": {},
   "source": [
    "#### 순방향 역방향 Weight를 각각 정의하므로 RNN의 2배 크기 Weight가 정의됨.\n",
    "units를 64로 정의했고, 입력은 Embedding을 포함하여 (1, 5, 100), 그리고 양방향에 대한 Weight를 거쳐 나올 테니 출력은 (1, 5, 128)이 나오게 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
