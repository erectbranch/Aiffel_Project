{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af0462b",
   "metadata": {},
   "source": [
    "# TensorFlow 2 API로 모델 구성하기 / CIFAR-100 적용편\n",
    "---\n",
    "#### 3가지 방법을 모두 적용\n",
    "* Sequential\n",
    "* Functional : Sequential의 일반화된 개념\n",
    "* Model Subclassing : 클래스로 구현된 기존의 모델을 상속받아 자신만의 모델을 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d877caf",
   "metadata": {},
   "source": [
    "## 1. Sequential API\n",
    "</br>\n",
    "\n",
    "\n",
    "### 1-1. 데이터 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1401afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c1c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169001437/169001437 [==============================] - 138s 1us/step\n",
      "50000 10000\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성부분\n",
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(len(x_train), len(x_test))\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35dbf2",
   "metadata": {},
   "source": [
    "### 1-2. Model 구성\n",
    "\n",
    ">Spec:\n",
    ">>1. 16개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    ">>2. pool_size가 2인 MaxPool 레이어\n",
    ">>3. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    ">>4. pool_size가 2인 MaxPool 레이어\n",
    ">>5. Flatten 레이어\n",
    ">>6. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    ">>7. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4696969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 00:54:35.218700: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-18 00:54:35.218915: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(16, 3, activation='relu'),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82159fb4",
   "metadata": {},
   "source": [
    "### 1-3. 모델 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95143a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 00:54:41.030265: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-09-18 00:54:41.288478: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 13s 8ms/step - loss: 3.6377 - accuracy: 0.1512\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 2.9827 - accuracy: 0.2678\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 2.6986 - accuracy: 0.3224\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 2.4979 - accuracy: 0.3645\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 2.3306 - accuracy: 0.3993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 00:55:40.295953: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 2.6496 - accuracy: 0.3387 - 1s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6495606899261475, 0.3387000262737274]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3c3bf",
   "metadata": {},
   "source": [
    "#### + model.compile의 속성을 따로 정의하고 넣어준 같은 코드\n",
    "\n",
    "```Python\n",
    "# 1.loss function(손실함수) 만들기\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# 2.optimizer 만들기(여기선 Adam 사용)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=2)\n",
    "```\n",
    "* from_logits=False : \n",
    "<br>* Softmax 함수를 사용하여 출력값으로 **해당 클래스의 범위에서의 확률**을 출력했으므로 from_logits=False (대체로 class 분류 문제)\n",
    "<br>* 반대로 모델의 출력값이 sigmoid 또는 linear를 거쳐서 **확률이 아닌 값**이 나오게 된다면, from_logits=True\n",
    "<br><br>\n",
    "\n",
    "\n",
    "* 참고\n",
    ">[from_logits란?](https://hwiyong.tistory.com/335)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3867bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# 메모리를 위한 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fa5bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Functional API\n",
    "</br>\n",
    "\n",
    "\n",
    "### 2-1. 데이터 호출(1-1과 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ea734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad840c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(len(x_train), len(x_test))\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eafb21",
   "metadata": {},
   "source": [
    "### 2-2. Model 구성\n",
    "\n",
    ">Spec:\n",
    ">>1. Input Laper: shape=\n",
    ">>2. Flatten 레이어\n",
    ">>3. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    ">>4. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34ea3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "\n",
    "x = keras.layers.Conv2D(16, 3, activation='relu')(inputs)\n",
    "x = keras.layers.MaxPool2D((2,2))(x)\n",
    "x = keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = keras.layers.MaxPool2D((2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(100, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563451a2",
   "metadata": {},
   "source": [
    "### 2-3. 모델 학습 설정(1-3과 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f17e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  15/1563 [..............................] - ETA: 12s - loss: 4.6179 - accuracy: 0.0104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 01:05:30.119713: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 12s 8ms/step - loss: 3.6229 - accuracy: 0.1583\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 2.8948 - accuracy: 0.2874\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 2.5889 - accuracy: 0.3466\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 2.3876 - accuracy: 0.3885\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 2.2181 - accuracy: 0.4235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 01:06:29.874322: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 2.5724 - accuracy: 0.3534 - 1s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5723557472229004, 0.35340002179145813]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38e91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# 메모리 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc160b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Subclassing 활용\n",
    "</br>\n",
    "\n",
    "\n",
    "### 3-2. Model 구성만 진행(3-1과 3-3은 위와 동일)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a12e1",
   "metadata": {},
   "source": [
    ">Spec:\n",
    ">>0. keras.Model 을 상속받았으며, __init__()와 call() 메서드를 가진 모델 클래스\n",
    ">>1. self.flatten = Flatten 레이어\n",
    ">>2. self.linear = 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    ">>3. self.linear2 = 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n",
    ">>4. call의 입력값이 모델의 Input, call의 리턴값이 모델의 Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24a8dd",
   "metadata": {},
   "source": [
    "```Python\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(16, 3, activation='relu')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D((2,2))\n",
    "        self.conv2 = keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D((2,2))\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(100, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "model = CustomModel()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f147441",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 심화 : GradientTape 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02d7e3",
   "metadata": {},
   "source": [
    "지금까지 3가지 방식으로 모델을 작성했지만 모델 학습 부분은 동일하게 진행하였음\n",
    "```Python\n",
    "# 모델 학습 설정\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc7f1f",
   "metadata": {},
   "source": [
    "### 그렇다면 model.fit() 내부에서 실제로 훈련이 되는 과정은?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7d7d5",
   "metadata": {},
   "source": [
    "> 1. Forward Propagation 수행 및 중간 레이어값 저장\n",
    "> 2. Loss 값 계산\n",
    "> 3. 중간 레이어값 및 Loss를 활용한 체인룰(chain rule) 방식의 역전파(Backward Propagation) 수행\n",
    "> 4. 학습 파라미터 업데이트\n",
    "\n",
    "이상의 4단계로 이루어진 train_step을 여러 번 반복하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc39c90",
   "metadata": {},
   "source": [
    "### * 여기서 GradientTape의 개념이 등장\n",
    "> 순전파(forward pass)로 진행된 모든 연산의 중간 레이어값을 **tape**에 기록하고, 이를 이용해 gradient를 계산한 뒤 **tape**를 폐기\n",
    " \n",
    "backpropagation을 할 때 forward propagation에서 gradient 값들을 저장해두면 이를 이용해 훨씬 빠르게 연산을 할 수 있음. 따라서 forward propagation이 진행되는 동안 그 값들을 저장할 필요가 있음. 즉, prediction을 구하는 과정과 loss를 구하는 과정이 tf.GradientTape의 대상이 되는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd97c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# 앞서 구성한 Subclassing 모델\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# 데이터 구성부분\n",
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(len(x_train), len(x_test))\n",
    "\n",
    "# 모델 구성부분\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(16, 3, activation='relu')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D((2,2))\n",
    "        self.conv2 = keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D((2,2))\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(100, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106618bd",
   "metadata": {},
   "source": [
    "### GradientTape를 적용한 모델 학습\n",
    "\n",
    "tape.gradient()를 통해 매 스텝 학습이 진행될 때마다 발생하는 그래디언트를 추출한 후 optimizer.apply_gradients()를 통해 발생한 그래디언트가 업데이트해야 할 파라미터 model.trainable_variables를 지정해 주는 과정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9e439",
   "metadata": {},
   "source": [
    "비교 : (적용하지 않았을 때)\n",
    "```Python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "#### * step마다 gradient를 추출하는 함수를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cb16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# tf.GradientTape()를 활용한 train_step\n",
    "def train_step(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features) #미니배치만큼 데이터를 가져온 뒤 경사를 초기화한 후 순방향전파\n",
    "        loss = loss_func(labels, predictions) #손실을 계산\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779136b",
   "metadata": {},
   "source": [
    "이렇게 매 스텝 진행되는 학습의 실제 동작이 train_step() 메서드로 구현됨\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### * tape를 적용하고 model.fit()를 대체하는 함수 작성\n",
    "model.fit()으로 위와 같이 한 줄로 간단하게 수행되던 실제 배치 학습 과정은, 매 스텝마다 위에서 구현했던 train_step()가 호출되는 과정으로 바꾸어 구현할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93acc255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: last batch loss = 3.2754\n",
      "Epoch 1: last batch loss = 2.6648\n",
      "Epoch 2: last batch loss = 2.3342\n",
      "Epoch 3: last batch loss = 2.0787\n",
      "Epoch 4: last batch loss = 1.9425\n",
      "It took 77.94227981567383 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train_model(batch_size=32):\n",
    "    start = time.time()\n",
    "    for epoch in range(5):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for step, (x, y) in enumerate(zip(x_train, y_train)):\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "            if step % batch_size == batch_size-1:\n",
    "                loss = train_step(np.array(x_batch, dtype=np.float32), np.array(y_batch, dtype=np.float32))\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "        print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))\n",
    "    print(\"It took {} seconds\".format(time.time() - start))\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff79c5",
   "metadata": {},
   "source": [
    "#### => 위에서 구현한 train_model() 메서드가 그동안 사용했던 model.fit() 메서드와 기능적으로 같은 것을 확인할 수 있다.\n",
    "**이렇듯 tf.GradientTape()를 활용하면 model.compile()과 model.fit() 안에 감추어져 있던 한 스텝의 학습 단계(위 예제에서는 train_step 메서드)를 끄집어내서 자유롭게 재구성할 수 있음.**<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "여러 다른 강화학습 또는 GAN(Generative Advasarial Network)의 학습을 위해서는 train_step 메서드의 재구성이 필수적이므로 tf.GradientTape()의 활용법을 꼭 숙지해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5cd347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 01:30:59.121117: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 315ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3413"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가 단계\n",
    "# gradient가 필요 없으므로 기존의 model.predict() 메서드를 이용\n",
    "\n",
    "prediction = model.predict(x_test, batch_size=x_test.shape[0], verbose=1)\n",
    "temp = sum(np.squeeze(y_test) == np.argmax(prediction, axis=1))\n",
    "temp/len(y_test)  # Accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
